{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SubhamPanda2003/Machine_learning/blob/master/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "YyNFLUuS_YNH",
        "outputId": "7325da11-299c-4bf1-8842-9390df597b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:148: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTypeError: float() argument must be a string or a number, not 'list'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 47, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"<ipython-input-82-5c2c552d7324>\", line 62, in predict\n    svm_model_linear = SVC(kernel = 'rbf', C = 1).fit(X, y)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\", line 196, in fit\n    accept_large_sparse=False,\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 581, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 976, in check_X_y\n    estimator=estimator,\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 746, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\n  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\", line 1993, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: setting an array element with a sequence.\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-5c2c552d7324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-82-5c2c552d7324>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-5c2c552d7324>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, test_set, training_set)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# The training data gets repeated to get an iterable of the training dataset for the map function, ie. the predict funtion, to be applied on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                         \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         '''\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ],
      "source": [
        "#================================================================================================================\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "#\t\t\t\t\t\t\t\t\tK NEAREST NEIGHBOURS\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "#================================================================================================================\n",
        "\n",
        "# Details of implementation/tutorial is in : http://madhugnadig.com/articles/machine-learning/parallel-processing/2017/02/10/implementing-k-nearest-neighbours-in-parallel-from-scratch.html\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn import preprocessing\n",
        "from itertools import repeat\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#for plotting\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "class CustomKNN:\n",
        "\t\n",
        "\tdef __init__(self):\n",
        "\t\tself.accurate_predictions = 0\n",
        "\t\tself.total_predictions = 0\n",
        "\t\tself.accuracy = 0.0\n",
        "\n",
        "\tdef predict(self, training_data, to_predict, k = 3):\n",
        "\t\tif len(training_data) >= k:\n",
        "\t\t\tprint(\"K cannot be smaller than the total voting groups(ie. number of training data points)\")\n",
        "\t\t\treturn\n",
        "\t\t\n",
        "\t\tdistributions = []\n",
        "\t\tfor group in training_data:\n",
        "\t\t\tfor features in training_data[group]:\n",
        "\t\t\t\t# Find euclidean distance using the numpy function\n",
        "\t\t\t\teuclidean_distance = np.linalg.norm(np.array(features)- np.array(to_predict))\n",
        "\t\t\t\tdistributions.append([euclidean_distance,group, features])\n",
        "\t\t# Find the k nearest neighbors\n",
        "\t\tresults = [i[1] for i in sorted(distributions)[:k]]\n",
        "\t\tres = [i for i in sorted(distributions)[:k]]\n",
        "\t\t# Figure out which is the most common class amongst the neighbors.\n",
        "\t\t# result = Counter(results).most_common(1)[0][0]\n",
        "\t\t# confidence = Counter(results).most_common(1)[0][1]/k\n",
        "\t\tiris1 = pd.DataFrame.from_dict(res)\n",
        "\t\tiris2 = pd.DataFrame.from_dict(results)\n",
        "# X -> features, y -> label\n",
        "\t\tX = iris1\n",
        "\t\ty = iris2\n",
        "  \n",
        "# dividing X, y into train and test data\n",
        "\t\t# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "  \n",
        "# training a linear SVM classifier\n",
        "\t\tfrom sklearn.svm import SVC\n",
        "\t\tsvm_model_linear = SVC(kernel = 'rbf', C = 1).fit(X, y)\n",
        "\t\tresult = svm_model_linear.predict(to_predict)\n",
        "# \t\tprint(distributions)\n",
        "\t\t# print(res)\n",
        "\t\treturn result, to_predict\n",
        "\t\n",
        "\t\n",
        "\tdef test(self, test_set, training_set):\n",
        "\t\tpool = mp.Pool(processes= 8)\n",
        "\n",
        "\t\tarr = {}\n",
        "\t\ts = time.clock()\n",
        "\t\t\n",
        "\t\t# Where the magic happens, this is where we parallelize our code. While testing for the classes of incoming points,\n",
        "\t\t# we divide the incoming data points and feed them into the predict funtion in parallel.\n",
        "\t\t# I have used the starpmap function of multiprocessing library for this purpose. \n",
        "\t\t# The training data gets repeated to get an iterable of the training dataset for the map function, ie. the predict funtion, to be applied on.\n",
        "\t\tfor group in test_set:\n",
        "\t\t\tarr[group] =  pool.starmap(self.predict, zip(repeat(training_set), test_set[group], repeat(3)))\n",
        "\t\te = time.clock()\n",
        "\n",
        "\t\t#Calculating Accuracy - The accuracy code has to be modified due to the induced parallelism. \n",
        "\t\t# It is no longer possible to determinstically calculate the accurate predictions where multiple subprocesses are doing the same increment.\n",
        "\n",
        "\t\tfor group in test_set:\n",
        "\t\t\tfor data in test_set[group]:\n",
        "\t\t\t\tfor i in arr[group]:\n",
        "\t\t\t\t\tif data == i[1]:\n",
        "\t\t\t\t\t\tself.total_predictions += 1\n",
        "\t\t\t\t\t\tif group == i[0]:\n",
        "\t\t\t\t\t\t\tself.accurate_predictions+=1\n",
        "\t\t\n",
        "\t\tself.accuracy = 100*(self.accurate_predictions/self.total_predictions)\n",
        "\t\tprint(\"\\nAcurracy :\", str(self.accuracy) + \"%\")\n",
        "\n",
        "def mod_data(df):\n",
        "\tdf.replace('?', -999999, inplace = True)\n",
        "\t\n",
        "\tdf.replace('yes', 4, inplace = True)\n",
        "\tdf.replace('no', 2, inplace = True)\n",
        "\n",
        "\tdf.replace('notpresent', 4, inplace = True)\n",
        "\tdf.replace('present', 2, inplace = True)\n",
        "\t\n",
        "\tdf.replace('abnormal', 4, inplace = True)\n",
        "\tdf.replace('normal', 2, inplace = True)\n",
        "\t\n",
        "\tdf.replace('poor', 4, inplace = True)\n",
        "\tdf.replace('good', 2, inplace = True)\n",
        "\t\n",
        "\tdf.replace('ckd', 4, inplace = True)\n",
        "\tdf.replace('notckd', 2, inplace = True)\n",
        "\n",
        "def main():\n",
        "\tdf = pd.read_csv(r\"./chronic_kidney_disease.csv\")\n",
        "\tmod_data(df)\n",
        "\tdataset = df.astype(float).values.tolist()\n",
        "\t\n",
        "\t#Normalize the data\n",
        "\tx = df.values #returns a numpy array\n",
        "\tmin_max_scaler = preprocessing.MinMaxScaler()\n",
        "\tx_scaled = min_max_scaler.fit_transform(x)\n",
        "\tdf = pd.DataFrame(x_scaled) #Replace df with normalized values\n",
        "\t\n",
        "\t#Shuffle the dataset\n",
        "\trandom.shuffle(dataset)\n",
        "\n",
        "\t#20% of the available data will be used for testing\n",
        "\ttest_size = 0.1\n",
        "\n",
        "\t#The keys of the dict are the classes that the data is classfied into\n",
        "\ttraining_set = {2: [], 4:[]}\n",
        "\ttest_set = {2: [], 4:[]}\n",
        "\t\n",
        "\t#Split data into training and test for cross validation\n",
        "\ttraining_data = dataset[:-int(test_size * len(dataset))]\n",
        "\ttest_data = dataset[-int(test_size * len(dataset)):]\n",
        "\t\n",
        "\t#Insert data into the training set\n",
        "\tfor record in training_data:\n",
        "\t\ttraining_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n",
        "\n",
        "\t#Insert data into the test set\n",
        "\tfor record in test_data:\n",
        "\t\ttest_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n",
        "\t\n",
        "\ts = time.clock()\n",
        "\tknn = CustomKNN()\n",
        "\tknn.test(test_set, training_set)\n",
        "\te = time.clock()\n",
        "\t\n",
        "\tprint(\"Exec Time: \", e-s)\n",
        "\t\n",
        "if __name__ == \"__main__\":\n",
        "\tmain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKYMxwKn_Z2q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpO+L3MoCRaDg4ZuJGBaI3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}